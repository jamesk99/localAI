# Example Environment Configuration for RAG System
# Copy this file to .env and customize for your hardware
 
 # SECURITY NOTE:
 # - This file is intentionally safe to commit to a public repo.
 # - Do NOT put real secrets in this file.
 # - Put real credentials/tokens in a local .env file (which should be gitignored).

# ============================================================================
# HARDWARE CONFIGURATION
# ============================================================================

# ROCm GPU Acceleration (AMD)
USE_ROCM=false              # Set to true when on GMKtec EVO-X2 with ROCm installed
USE_NPU=false               # NPU support (future feature)
GPU_LAYERS=0                # Number of layers to offload to GPU (0 = auto)
NUM_GPU=1                   # Number of GPUs to use

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================

# LLM Models (via Ollama)
LLM_MODEL=llama3:latest                      # Primary model
LLM_FALLBACK=deepseek-r1:latest             # Fallback if primary fails
EMBED_MODEL=nomic-embed-text                # Embedding model

# For Phase 1 on new hardware, use:
# LLM_MODEL=qwen2.5:72b-instruct-q4_K_M
# LLM_FALLBACK=llama3.1:70b-instruct-q4_K_M
# EMBED_MODEL=bge-large

# ============================================================================
# LLM PARAMETERS
# ============================================================================

LLM_TEMPERATURE=0.1         # Lower = more focused, higher = more creative
LLM_CONTEXT_WINDOW=8192     # Context window size (8K, 32K, 64K, 128K)
LLM_REQUEST_TIMEOUT=180.0   # Request timeout in seconds
LLM_NUM_PREDICT=512         # Maximum tokens to generate per response

# ============================================================================
# RAG CONFIGURATION
# ============================================================================

# Chunking
CHUNK_SIZE=1024             # Chunk size in tokens
CHUNK_OVERLAP=128           # Overlap between chunks

# Retrieval
TOP_K=5                     # Number of chunks to retrieve
SIMILARITY_THRESHOLD=0.3    # Minimum similarity score (0.0-1.0)
MAX_CHUNKS_IN_CONTEXT=10    # Maximum chunks to include in prompt

# Advanced (Phase 2+)
USE_RERANKING=false         # Enable reranking (requires additional setup)
RERANK_TOP_N=3              # Number of results to rerank

# ============================================================================
# OLLAMA CONFIGURATION
# ============================================================================

OLLAMA_BASE_URL=http://localhost:11434

# ============================================================================
# WEB APPLICATION
# ============================================================================

RAG_APP_HOST=127.0.0.1      # Host to bind to (127.0.0.1 = local-only; use 0.0.0.0 only if you understand exposure)
RAG_APP_PORT=8000           # Port to listen on
FLASK_DEBUG=false           # Enable debug mode (development only)

# ============================================================================
# AUTHENTICATION
# ============================================================================

# Single user mode (if no auth_users.json file exists)
RAG_USER=
RAG_PASS=

# Multi-user mode: Create src/auth_users.json instead
# RAG_USER_FILE=src/auth_users.json

# ============================================================================
# LOGGING
# ============================================================================

RAG_LOG_LEVEL=INFO          # DEBUG, INFO, WARNING, ERROR
RAG_LOG_FILE=logs/rag_app.log
RAG_LOG_QUESTIONS=false     # Log user questions (set true only if you explicitly want this)

# ============================================================================
# PATHS (usually don't need to change)
# ============================================================================

# DATA_DIR=data
# RAW_DOCS_DIR=data/raw
# VECTOR_DB_DIR=data/vectordb
# TRACKING_DB_PATH=data/tracking.db
